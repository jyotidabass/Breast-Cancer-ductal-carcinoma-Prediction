{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting invasive ductal carcinoma in tissue slices\n\n\n## Motivation\n\nInasive ductal carcinoma (IDC) is - with ~ 80 % of cases - one of the most common types of breast cancer. It's malicious and able to form metastases which makes it especially dangerous. Often a biopsy is done to remove small tissue samples. Then a pathologist has to decide whether a patient has IDC, another type of breast cancer or is healthy. In addition sick cells need to be located to find out how advanced the disease is and which grade should be assigned. This has to be done manually and is a time consuming process. Furthermore the decision depends on the expertise of the pathologist and his or her equipment. Therefor deep learning could be of great help to automatically detect and locate tumor tissue cells and to speed up the process. In order to exploit the full potential one could build a pipeline using massive amounts of tissue image data of various hospitals that were evaluated by different experts. This way one would be able to overcome the dependence on the pathologist which would be especially useful in regions where no experts are available .      \n\n## Our goal\n\nAs we started with this analysis we asked ourselves if we would be able to improve the results that were presented 2014 in the paper [Automatic detection of invasive ductal carcinoma in whole slide images with Convolutional Neural Networks](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.725.4294&rep=rep1&type=pdf) of professor [Anant Madabhushi](https://case.edu/medicine/ccir/faculty/anant-madabhushi) and his group. Many years have passed by since then and it's very likely that all methods used in the paper have already been changed, improved and that new research has already been done. Nonetheless it's a very good exercise to practice or develop own deep learning and data science skills.\n\n\n## Methods presented in the paper\n\nCollecting information... ;-)\n\n* In the paper tissue slices of 162 patients were used all having IDC (113 used for training and 49 for validation)\n* One pathologist was used to determine regions of IDC given a tissue slice \n* evaluation metric: F1 score and balanced accuracy\n\n\n* Our goal: Given a patient and a patch of a tissue slice predict wheather it contains IDC or not.\n    * 3 possibilities: healthy tissue, IDC, another subtype of breast cancer\n* business case: prediction so far is done manually by pathologists and varies from expert to expert. The goal is to assist with an automatic detection of tumors (not expert dependent). \n\n\n## Table of contents\n\n1. [What is meant by invasive ductal carcinoma?](#intro) \n2. [Preparation & peek at the data structure](#prep)\n    * [Loading packages and settings](#setup)\n    * [Exploring the data structure](#explorestructure)\n3. [Exploratory analysis](#eda)\n    * [What do we know about our data?](#data)\n    * [Looking at healthy and invasive ductal carcinoma patches](#patches)\n    * [Visualising the breast tissue](#tissue)\n4. [Setting up the machine learning workflow](#workflow)\n    * [Settings](#ml_settings)\n    * [Validation strategy](#validation)\n    * [Target distributions](#target_dists)\n    * [Creating pytorch image datasets](#image_datasets)\n    * [Creating pytorch dataloaders](#dataloaders)\n    * [Defining the model structure](#model_structure)\n    * [Setting up the loss function](#loss_eva)\n    * [Selecting an evaluation metric](#e_metric)\n    * [Building the training loop](#train_loop)\n    * [Searching for an optimal cyclical learning rate](#lr_cycle_optima)\n    * [Performing the training or loading results](#run)\n5. [Exploring results and errors](#error_analysis)\n    * [Loss convergence](#losses)\n    * [The probability landscape of invasive ductal carcinoma](#landscape)\n    * [Going into details](#details)\n7. [Conclusion](#conclusion)\n","metadata":{}},{"cell_type":"markdown","source":"## What is meant by invasive ductal carcinoma? <a class=\"anchor\" id=\"intro\"></a>\n\n<a title=\"Mikael Häggström, M.D. - Author info - Reusing images [CC BY (https://creativecommons.org/licenses/by/2.5)]\" href=\"https://commons.wikimedia.org/wiki/File:Lobules_and_ducts_of_the_breast.jpg\"><img width=\"309\" alt=\"Lobules and ducts of the breast\" style=\"float:left; margin:0px 15px 15px 15px\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Lobules_and_ducts_of_the_breast.jpg/256px-Lobules_and_ducts_of_the_breast.jpg\"></a>\n\n\nThis illustration created [Mikael Häggström](https://commons.wikimedia.org/wiki/File:Lobules_and_ducts_of_the_breast.jpg) shows the anatomy of a healthy breast. One can see the lobules, the glands that can produce milk which flews through the milk ducts. Ductal carcinoma starts to develop in the ducts whereas lobular carcinoma has its origin in the lobules. Invasive carcinoma is able to leave its initial tissue compartment and can form metastases. ","metadata":{}},{"cell_type":"markdown","source":"# Preparation & peek at the data structure <a class=\"anchor\" id=\"prep\"></a>\n\n## Loading packages and setting <a class=\"anchor\" id=\"setup\"></a>","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CyclicLR\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\n\n\nfrom glob import glob\nfrom skimage.io import imread\nfrom os import listdir\n\nimport time\nimport copy\nfrom tqdm import tqdm_notebook as tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:26:54.779426Z","iopub.execute_input":"2022-08-04T06:26:54.780058Z","iopub.status.idle":"2022-08-04T06:26:58.580555Z","shell.execute_reply.started":"2022-08-04T06:26:54.779991Z","shell.execute_reply":"2022-08-04T06:26:58.579493Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Settings","metadata":{}},{"cell_type":"code","source":"run_training = False\nretrain = False\nfind_learning_rate = False","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:26:58.582669Z","iopub.execute_input":"2022-08-04T06:26:58.582950Z","iopub.status.idle":"2022-08-04T06:26:58.587504Z","shell.execute_reply.started":"2022-08-04T06:26:58.582900Z","shell.execute_reply":"2022-08-04T06:26:58.586577Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Exploring the data structure <a class=\"anchor\" id=\"explorestructure\"></a>","metadata":{}},{"cell_type":"code","source":"files = listdir(\"../input/breast-histopathology-images/\")\nprint(len(files))","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:26:58.588974Z","iopub.execute_input":"2022-08-04T06:26:58.589532Z","iopub.status.idle":"2022-08-04T06:26:58.678215Z","shell.execute_reply.started":"2022-08-04T06:26:58.589471Z","shell.execute_reply":"2022-08-04T06:26:58.677195Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"files[0:10]","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:26:58.679332Z","iopub.execute_input":"2022-08-04T06:26:58.679746Z","iopub.status.idle":"2022-08-04T06:26:58.686779Z","shell.execute_reply.started":"2022-08-04T06:26:58.679705Z","shell.execute_reply":"2022-08-04T06:26:58.685832Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Ok, in this folder we should find several images or a further substructure of folders.","metadata":{}},{"cell_type":"code","source":"files = listdir(\"../input/breast-histopathology-images/IDC_regular_ps50_idx5/\")\nlen(files)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:26:58.690584Z","iopub.execute_input":"2022-08-04T06:26:58.691198Z","iopub.status.idle":"2022-08-04T06:26:58.764384Z","shell.execute_reply.started":"2022-08-04T06:26:58.691150Z","shell.execute_reply":"2022-08-04T06:26:58.763372Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Ah ok. These are patient ids. For each patient we have an individual subfolder that contains image patches. \n\n### How many patients do we have?","metadata":{}},{"cell_type":"code","source":"base_path = \"../input/breast-histopathology-images/IDC_regular_ps50_idx5/\"\nfolder = listdir(base_path)\nlen(folder)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:26:58.768418Z","iopub.execute_input":"2022-08-04T06:26:58.768736Z","iopub.status.idle":"2022-08-04T06:26:58.774698Z","shell.execute_reply.started":"2022-08-04T06:26:58.768678Z","shell.execute_reply":"2022-08-04T06:26:58.773863Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Almost 280 patients. That's a small number compared to the expected number of patients one would like to analyse with our algorithm after deployment. **Consequently overfitting to this specific patient distribution is very likely and we need to take care about the generalization performance of our model**. ","metadata":{}},{"cell_type":"markdown","source":"### How many patches do we have in total?\n\nOur algorithm needs to decide whether an image patch contains IDC or not. Consequently not the whole patient tissue slice but the single patches have to be considered as input to our algorithm. How many of them do we have in total?","metadata":{}},{"cell_type":"code","source":"total_images = 0\nfor n in range(len(folder)):\n    patient_id = folder[n]\n    for c in [0, 1]:\n        patient_path = base_path + patient_id \n        class_path = patient_path + \"/\" + str(c) + \"/\"\n        subfiles = listdir(class_path)\n        total_images += len(subfiles)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:26:58.775969Z","iopub.execute_input":"2022-08-04T06:26:58.776496Z","iopub.status.idle":"2022-08-04T06:28:19.808874Z","shell.execute_reply.started":"2022-08-04T06:26:58.776439Z","shell.execute_reply":"2022-08-04T06:28:19.807796Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"total_images","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:28:19.810298Z","iopub.execute_input":"2022-08-04T06:28:19.810652Z","iopub.status.idle":"2022-08-04T06:28:19.818524Z","shell.execute_reply.started":"2022-08-04T06:28:19.810586Z","shell.execute_reply":"2022-08-04T06:28:19.815551Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Ok, roughly 280000 images. To feed the algorithm with image patches it would be nice to store the path of each image. This way we can load batches of images only one by one without storing the pixel values of all images. ","metadata":{}},{"cell_type":"markdown","source":"### Storing the image_path, patient_id and the target","metadata":{}},{"cell_type":"code","source":"data = pd.DataFrame(index=np.arange(0, total_images), columns=[\"patient_id\", \"path\", \"target\"])\n\nk = 0\nfor n in range(len(folder)):\n    patient_id = folder[n]\n    patient_path = base_path + patient_id \n    for c in [0,1]:\n        class_path = patient_path + \"/\" + str(c) + \"/\"\n        subfiles = listdir(class_path)\n        for m in range(len(subfiles)):\n            image_path = subfiles[m]\n            data.iloc[k][\"path\"] = class_path + image_path\n            data.iloc[k][\"target\"] = c\n            data.iloc[k][\"patient_id\"] = patient_id\n            k += 1  \n\ndata.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:28:19.820874Z","iopub.execute_input":"2022-08-04T06:28:19.822190Z","iopub.status.idle":"2022-08-04T06:30:22.471716Z","shell.execute_reply.started":"2022-08-04T06:28:19.821190Z","shell.execute_reply":"2022-08-04T06:30:22.470689Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Ok, now for each patient we know the path for each patch as well as if it contains IDC or not (the target).","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:22.473125Z","iopub.execute_input":"2022-08-04T06:30:22.473425Z","iopub.status.idle":"2022-08-04T06:30:22.478809Z","shell.execute_reply.started":"2022-08-04T06:30:22.473361Z","shell.execute_reply":"2022-08-04T06:30:22.477865Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"No surprise. This matches the total number of patches. ","metadata":{}},{"cell_type":"markdown","source":"# Exploratory analysis <a class=\"anchor\" id=\"eda\"></a>\n\n## What do we know about our data? <a class=\"anchor\" id=\"data\"></a>","metadata":{}},{"cell_type":"code","source":"cancer_perc = data.groupby(\"patient_id\").target.value_counts()/ data.groupby(\"patient_id\").target.size()\ncancer_perc = cancer_perc.unstack()\n\nfig, ax = plt.subplots(1,3,figsize=(20,5))\nsns.distplot(data.groupby(\"patient_id\").size(), ax=ax[0], color=\"Orange\", kde=False, bins=30)\nax[0].set_xlabel(\"Number of patches\")\nax[0].set_ylabel(\"Frequency\");\nax[0].set_title(\"How many patches do we have per patient?\");\nsns.distplot(cancer_perc.loc[:, 1]*100, ax=ax[1], color=\"Tomato\", kde=False, bins=30)\nax[1].set_title(\"How much percentage of an image is covered by IDC?\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_xlabel(\"% of patches with IDC\");\nsns.countplot(data.target, palette=\"Set2\", ax=ax[2]);\nax[2].set_xlabel(\"no(0) versus yes(1)\")\nax[2].set_title(\"How many patches show IDC?\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:22.480233Z","iopub.execute_input":"2022-08-04T06:30:22.480535Z","iopub.status.idle":"2022-08-04T06:30:23.369227Z","shell.execute_reply.started":"2022-08-04T06:30:22.480482Z","shell.execute_reply":"2022-08-04T06:30:23.368316Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Insights\n\n1. The number of image patches per patient varies a lot! **This leads to the questions whether all images show the same resolution of tissue cells of if this varies between patients**. \n2. Some patients have more than 80 % patches that show IDC! Consequently the tissue is full of cancer or only a part of the breast was covered by the tissue slice that is focused on the IDC cancer. **Does a tissue slice per patient cover the whole region of interest?**\n3. The **classes of IDC versus no IDC are imbalanced**. We have to check this again after setting up a validation strategy and find a strategy to deal with class weights (if we like to apply them).","metadata":{}},{"cell_type":"markdown","source":"## Looking at healthy and cancer patches <a class=\"anchor\" id=\"patches\"></a>","metadata":{}},{"cell_type":"code","source":"data.target = data.target.astype(np.int)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:23.370812Z","iopub.execute_input":"2022-08-04T06:30:23.371129Z","iopub.status.idle":"2022-08-04T06:30:23.412166Z","shell.execute_reply.started":"2022-08-04T06:30:23.371072Z","shell.execute_reply":"2022-08-04T06:30:23.411227Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"pos_selection = np.random.choice(data[data.target==1].index.values, size=50, replace=False)\nneg_selection = np.random.choice(data[data.target==0].index.values, size=50, replace=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:23.413666Z","iopub.execute_input":"2022-08-04T06:30:23.413950Z","iopub.status.idle":"2022-08-04T06:30:23.455997Z","shell.execute_reply.started":"2022-08-04T06:30:23.413901Z","shell.execute_reply":"2022-08-04T06:30:23.455140Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Cancer patches","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(5,10,figsize=(20,10))\n\nfor n in range(5):\n    for m in range(10):\n        idx = pos_selection[m + 10*n]\n        image = imread(data.loc[idx, \"path\"])\n        ax[n,m].imshow(image)\n        ax[n,m].grid(False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:23.457685Z","iopub.execute_input":"2022-08-04T06:30:23.458408Z","iopub.status.idle":"2022-08-04T06:30:28.777667Z","shell.execute_reply.started":"2022-08-04T06:30:23.458327Z","shell.execute_reply":"2022-08-04T06:30:28.776821Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Healthy patches","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(5,10,figsize=(20,10))\n\nfor n in range(5):\n    for m in range(10):\n        idx = neg_selection[m + 10*n]\n        image = imread(data.loc[idx, \"path\"])\n        ax[n,m].imshow(image)\n        ax[n,m].grid(False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:28.778877Z","iopub.execute_input":"2022-08-04T06:30:28.779291Z","iopub.status.idle":"2022-08-04T06:30:33.993315Z","shell.execute_reply.started":"2022-08-04T06:30:28.779249Z","shell.execute_reply":"2022-08-04T06:30:33.992553Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Insights\n\n* Sometimes we can find artifacts or incomplete patches that have smaller size than 50x50 pixels. \n* Patches with cancer look more violet and crowded than healthy ones. Is this really typical for cancer or is it more typical for ductal cells and tissue?\n* Though some of the healthy patches are very violet colored too!\n* Would be very interesting to hear what criteria are important for a pathologist.\n* I assume that the wholes in the tissue belong to the mammary ducts where the milk can flow through. ","metadata":{}},{"cell_type":"markdown","source":"## Visualising the breast tissue <a class=\"anchor\" id=\"tissue\"></a>\n\nThis part is a bit tricky! We have to extract all coordinates of image patches that are stored in the image names. Then we can use the coordinates to reconstruct the whole breast tissue of a patient. This way we can also explore how diseased tissue looks like compared to healthy ones. To simplify this task let's write a method that takes a patient and outcomes a dataframe with coordinates and targets.","metadata":{}},{"cell_type":"code","source":"def extract_coords(df):\n    coord = df.path.str.rsplit(\"_\", n=4, expand=True)\n    coord = coord.drop([0, 1, 4], axis=1)\n    coord = coord.rename({2: \"x\", 3: \"y\"}, axis=1)\n    coord.loc[:, \"x\"] = coord.loc[:,\"x\"].str.replace(\"x\", \"\", case=False).astype(np.int)\n    coord.loc[:, \"y\"] = coord.loc[:,\"y\"].str.replace(\"y\", \"\", case=False).astype(np.int)\n    df.loc[:, \"x\"] = coord.x.values\n    df.loc[:, \"y\"] = coord.y.values\n    return df\n\ndef get_cancer_dataframe(patient_id, cancer_id):\n    path = base_path + patient_id + \"/\" + cancer_id\n    files = listdir(path)\n    dataframe = pd.DataFrame(files, columns=[\"filename\"])\n    path_names = path + \"/\" + dataframe.filename.values\n    dataframe = dataframe.filename.str.rsplit(\"_\", n=4, expand=True)\n    dataframe.loc[:, \"target\"] = np.int(cancer_id)\n    dataframe.loc[:, \"path\"] = path_names\n    dataframe = dataframe.drop([0, 1, 4], axis=1)\n    dataframe = dataframe.rename({2: \"x\", 3: \"y\"}, axis=1)\n    dataframe.loc[:, \"x\"] = dataframe.loc[:,\"x\"].str.replace(\"x\", \"\", case=False).astype(np.int)\n    dataframe.loc[:, \"y\"] = dataframe.loc[:,\"y\"].str.replace(\"y\", \"\", case=False).astype(np.int)\n    return dataframe\n\ndef get_patient_dataframe(patient_id):\n    df_0 = get_cancer_dataframe(patient_id, \"0\")\n    df_1 = get_cancer_dataframe(patient_id, \"1\")\n    patient_df = df_0.append(df_1)\n    return patient_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:33.994666Z","iopub.execute_input":"2022-08-04T06:30:33.995107Z","iopub.status.idle":"2022-08-04T06:30:34.009070Z","shell.execute_reply.started":"2022-08-04T06:30:33.995064Z","shell.execute_reply":"2022-08-04T06:30:34.008006Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"example = get_patient_dataframe(data.patient_id.values[0])\nexample.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:34.010336Z","iopub.execute_input":"2022-08-04T06:30:34.010847Z","iopub.status.idle":"2022-08-04T06:30:34.069863Z","shell.execute_reply.started":"2022-08-04T06:30:34.010792Z","shell.execute_reply":"2022-08-04T06:30:34.068805Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Ok, now we have the coordinates for each patch, its path to load the image and its target information.","metadata":{}},{"cell_type":"markdown","source":"### Binary target visualisation per tissue slice <a class=\"anchor\" id=\"binarytissue\"></a>\n\nBefore we will take a look at the whole tissue let's keep it a bit simpler by looking at the target structure in the x-y-space for a handful of patients:","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(5,3,figsize=(20, 27))\n\npatient_ids = data.patient_id.unique()\n\nfor n in range(5):\n    for m in range(3):\n        patient_id = patient_ids[m + 3*n]\n        example_df = get_patient_dataframe(patient_id)\n        \n        ax[n,m].scatter(example_df.x.values, example_df.y.values, c=example_df.target.values, cmap=\"coolwarm\", s=20);\n        ax[n,m].set_title(\"patient \" + patient_id)\n        ax[n,m].set_xlabel(\"y coord\")\n        ax[n,m].set_ylabel(\"x coord\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:34.071419Z","iopub.execute_input":"2022-08-04T06:30:34.071686Z","iopub.status.idle":"2022-08-04T06:30:38.171953Z","shell.execute_reply.started":"2022-08-04T06:30:34.071639Z","shell.execute_reply":"2022-08-04T06:30:38.170927Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Insights\n\n* Sometimes we don't have the full tissue information. It seems that tissue patches have been discarded or lost during preparation. \n* Reading the paper (link!) that seems to be related to this data this could also be part of the preprocessing.","metadata":{}},{"cell_type":"markdown","source":"### Visualising the breast tissue images <a class=\"anchor\" id=\"tissueimages\"></a>\n\nOk, now it's time to go one step deeper with our EDA. Given the coordinates of image patches we could try to reconstruct the whole tissue image (not only the targets). ","metadata":{}},{"cell_type":"code","source":"def visualise_breast_tissue(patient_id, pred_df=None):\n    example_df = get_patient_dataframe(patient_id)\n    max_point = [example_df.y.max()-1, example_df.x.max()-1]\n    grid = 255*np.ones(shape = (max_point[0] + 50, max_point[1] + 50, 3)).astype(np.uint8)\n    mask = 255*np.ones(shape = (max_point[0] + 50, max_point[1] + 50, 3)).astype(np.uint8)\n    if pred_df is not None:\n        patient_df = pred_df[pred_df.patient_id == patient_id].copy()\n    mask_proba = np.zeros(shape = (max_point[0] + 50, max_point[1] + 50, 1)).astype(np.float)\n    \n    broken_patches = []\n    for n in range(len(example_df)):\n        try:\n            image = imread(example_df.path.values[n])\n            \n            target = example_df.target.values[n]\n            \n            x_coord = np.int(example_df.x.values[n])\n            y_coord = np.int(example_df.y.values[n])\n            x_start = x_coord - 1\n            y_start = y_coord - 1\n            x_end = x_start + 50\n            y_end = y_start + 50\n\n            grid[y_start:y_end, x_start:x_end] = image\n            if target == 1:\n                mask[y_start:y_end, x_start:x_end, 0] = 250\n                mask[y_start:y_end, x_start:x_end, 1] = 0\n                mask[y_start:y_end, x_start:x_end, 2] = 0\n            if pred_df is not None:\n                \n                proba = patient_df[\n                    (patient_df.x==x_coord) & (patient_df.y==y_coord)].proba\n                mask_proba[y_start:y_end, x_start:x_end, 0] = np.float(proba)\n\n        except ValueError:\n            broken_patches.append(example_df.path.values[n])\n    \n    \n    return grid, mask, broken_patches, mask_proba","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:38.173517Z","iopub.execute_input":"2022-08-04T06:30:38.173809Z","iopub.status.idle":"2022-08-04T06:30:38.189220Z","shell.execute_reply.started":"2022-08-04T06:30:38.173746Z","shell.execute_reply":"2022-08-04T06:30:38.188238Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Let's use an example patient with id 13616: ","metadata":{}},{"cell_type":"code","source":"example = \"13616\"\ngrid, mask, broken_patches,_ = visualise_breast_tissue(example)\n\nfig, ax = plt.subplots(1,2,figsize=(20,10))\nax[0].imshow(grid, alpha=0.9)\nax[1].imshow(mask, alpha=0.8)\nax[1].imshow(grid, alpha=0.7)\nax[0].grid(False)\nax[1].grid(False)\nfor m in range(2):\n    ax[m].set_xlabel(\"y-coord\")\n    ax[m].set_ylabel(\"y-coord\")\nax[0].set_title(\"Breast tissue slice of patient: \" + patient_id)\nax[1].set_title(\"Cancer tissue colored red \\n of patient: \" + patient_id);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:38.190323Z","iopub.execute_input":"2022-08-04T06:30:38.190666Z","iopub.status.idle":"2022-08-04T06:30:49.816702Z","shell.execute_reply.started":"2022-08-04T06:30:38.190610Z","shell.execute_reply":"2022-08-04T06:30:49.815562Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Insights\n\n* The tissue on the left is shown without target information.\n* The image on the right shows the same tissue but cancer is stained with intensive red color. \n* Comparing both images it seems that darker, more violet colored tissue has a higher chance to be cancer than those with rose color. \n* But as one can see it's not always the case. So we need to ask ourselves if violet tissue patches have more mammary ducts than rose ones. If this is true we have to be careful. Our model might start to learn that mammary ducts are always related to cancer! \n\nSometimes it's not possible to load an image patch as the path is ill defined. But in our case, we were able to load them all:","metadata":{}},{"cell_type":"code","source":"broken_patches","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:49.818351Z","iopub.execute_input":"2022-08-04T06:30:49.818738Z","iopub.status.idle":"2022-08-04T06:30:49.825241Z","shell.execute_reply.started":"2022-08-04T06:30:49.818677Z","shell.execute_reply":"2022-08-04T06:30:49.824266Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Setting up the machine learning workflow <a class=\"anchor\" id=\"workflow\"></a>","metadata":{}},{"cell_type":"markdown","source":"## Settings <a class=\"anchor\" id=\"ml_settings\"></a>","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\nNUM_CLASSES = 2\n\nOUTPUT_PATH = \"\"\nMODEL_PATH = \"../input/breastcancermodel/\"\nLOSSES_PATH = \"../input/breastcancermodel/\"","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:49.827120Z","iopub.execute_input":"2022-08-04T06:30:49.827504Z","iopub.status.idle":"2022-08-04T06:30:49.837024Z","shell.execute_reply.started":"2022-08-04T06:30:49.827443Z","shell.execute_reply":"2022-08-04T06:30:49.835946Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0)\nnp.random.seed(0)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:49.838580Z","iopub.execute_input":"2022-08-04T06:30:49.839143Z","iopub.status.idle":"2022-08-04T06:30:49.852169Z","shell.execute_reply.started":"2022-08-04T06:30:49.839070Z","shell.execute_reply":"2022-08-04T06:30:49.850989Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Validation strategy <a class=\"anchor\" id=\"validation\"></a>\n\nLet's start very simple by selecting 30 % of the patients as test data and the remaining 70 % for training and developing. This seems arbitrary and we should rethink this strategy in the next cycle of our datascience workflow. \n\nA better idea could be to cluster patients with dependence on the size of the tumor, the number of total patches and statistical quantities of area coverd by the patches. The reason behind that is that we would like to have test patients that cover a broad range of possible variations. Only then we can measure something like a generalisation performance.    ","metadata":{}},{"cell_type":"code","source":"data.head()\ndata.loc[:, \"target\"] = data.target.astype(np.str)\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:49.853969Z","iopub.execute_input":"2022-08-04T06:30:49.854673Z","iopub.status.idle":"2022-08-04T06:30:50.156274Z","shell.execute_reply.started":"2022-08-04T06:30:49.854611Z","shell.execute_reply":"2022-08-04T06:30:50.154189Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"patients = data.patient_id.unique()\n\ntrain_ids, sub_test_ids = train_test_split(patients,\n                                           test_size=0.3,\n                                           random_state=0)\ntest_ids, dev_ids = train_test_split(sub_test_ids, test_size=0.5, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:50.157880Z","iopub.execute_input":"2022-08-04T06:30:50.160485Z","iopub.status.idle":"2022-08-04T06:30:50.177580Z","shell.execute_reply.started":"2022-08-04T06:30:50.158187Z","shell.execute_reply":"2022-08-04T06:30:50.176763Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"So far we can't stratify on the targets as we are splitting on patient ids. If we would like to include some target information we would need to create a feature that allows us to generate some balance. ","metadata":{}},{"cell_type":"code","source":"print(len(train_ids)/patients.shape[0]*100, len(dev_ids)/patients.shape[0]*100, len(test_ids)/patients.shape[0]*100)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:50.178883Z","iopub.execute_input":"2022-08-04T06:30:50.179256Z","iopub.status.idle":"2022-08-04T06:30:50.187959Z","shell.execute_reply.started":"2022-08-04T06:30:50.179180Z","shell.execute_reply":"2022-08-04T06:30:50.187137Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Now it's 70 % train and 15 % for dev and test.","metadata":{}},{"cell_type":"code","source":"print(len(train_ids), len(dev_ids), len(test_ids))","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:50.189350Z","iopub.execute_input":"2022-08-04T06:30:50.189889Z","iopub.status.idle":"2022-08-04T06:30:50.203810Z","shell.execute_reply.started":"2022-08-04T06:30:50.189843Z","shell.execute_reply":"2022-08-04T06:30:50.202658Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_df = data.loc[data.patient_id.isin(train_ids),:].copy()\ntest_df = data.loc[data.patient_id.isin(test_ids),:].copy()\ndev_df = data.loc[data.patient_id.isin(dev_ids),:].copy()\n\ntrain_df = extract_coords(train_df)\ntest_df = extract_coords(test_df)\ndev_df = extract_coords(dev_df)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:50.205589Z","iopub.execute_input":"2022-08-04T06:30:50.206102Z","iopub.status.idle":"2022-08-04T06:30:52.666571Z","shell.execute_reply.started":"2022-08-04T06:30:50.205903Z","shell.execute_reply":"2022-08-04T06:30:52.665698Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Target distributions <a class=\"anchor\" id=\"target_dists\"></a>\n\nLet's take a look at the target distribution difference of the datasets: ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,3,figsize=(20,5))\nsns.countplot(train_df.target, ax=ax[0], palette=\"Reds\")\nax[0].set_title(\"Train data\")\nsns.countplot(dev_df.target, ax=ax[1], palette=\"Blues\")\nax[1].set_title(\"Dev data\")\nsns.countplot(test_df.target, ax=ax[2], palette=\"Greens\");\nax[2].set_title(\"Test data\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:52.667982Z","iopub.execute_input":"2022-08-04T06:30:52.668278Z","iopub.status.idle":"2022-08-04T06:30:53.170962Z","shell.execute_reply.started":"2022-08-04T06:30:52.668222Z","shell.execute_reply":"2022-08-04T06:30:53.169920Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"We can see that the test data has more cancer patches compared to healthy tissue patches than train or dev. We should keep this in mind!","metadata":{}},{"cell_type":"markdown","source":"## Creating pytorch image datasets <a class=\"anchor\" id=\"image_datasets\"></a>\n\nIt's often a good idea to start as simple as possible and to grow more complex while iterating through the solution. This way we prevent to build up models that are likely overfitted to the available data and we can find out useful ideas in a strategic manner instead of trying out every idea at random.\n\nThe simplest transformations we can do for each image are:\n\n* resizing the images to the desired input shape\n* performing horizontal and vertical flips\n\nIn our case the patches are of shape 50x50x3 and we could set this as our input shape. As CNNs are translational but not rotational invariant, it's a good idea to add flips during training. This way we increase the variety of our data in a meaningful way as each patch could be rotated as well on the tissue slice. As we are not looking at the whole tissue we are not loosing spatial connections between patches and it's not important that some neighboring patches are rotated in different directions.","metadata":{}},{"cell_type":"code","source":"def my_transform(key=\"train\", plot=False):\n    train_sequence = [transforms.Resize((50,50)),\n                      transforms.RandomHorizontalFlip(),\n                      transforms.RandomVerticalFlip()]\n    val_sequence = [transforms.Resize((50,50))]\n    if plot==False:\n        train_sequence.extend([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n        val_sequence.extend([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n        \n    data_transforms = {'train': transforms.Compose(train_sequence),'val': transforms.Compose(val_sequence)}\n    return data_transforms[key]","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-08-04T06:30:53.172425Z","iopub.execute_input":"2022-08-04T06:30:53.172690Z","iopub.status.idle":"2022-08-04T06:30:53.180666Z","shell.execute_reply.started":"2022-08-04T06:30:53.172643Z","shell.execute_reply":"2022-08-04T06:30:53.179748Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Furthermore we need to create a dataset that loads an image patch of a patient, converts it to RGB, performs the augmentation if it's desired and returns the image, the target, the patient id and the image coordinates.","metadata":{}},{"cell_type":"code","source":"class BreastCancerDataset(Dataset):\n    \n    def __init__(self, df, transform=None):\n        self.states = df\n        self.transform=transform\n      \n    def __len__(self):\n        return len(self.states)\n        \n    def __getitem__(self, idx):\n        patient_id = self.states.patient_id.values[idx]\n        x_coord = self.states.x.values[idx]\n        y_coord = self.states.y.values[idx]\n        image_path = self.states.path.values[idx] \n        image = Image.open(image_path)\n        image = image.convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        if \"target\" in self.states.columns.values:\n            target = np.int(self.states.target.values[idx])\n        else:\n            target = None\n            \n        return {\"image\": image,\n                \"label\": target,\n                \"patient_id\": patient_id,\n                \"x\": x_coord,\n                \"y\": y_coord}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:53.182097Z","iopub.execute_input":"2022-08-04T06:30:53.182378Z","iopub.status.idle":"2022-08-04T06:30:53.196837Z","shell.execute_reply.started":"2022-08-04T06:30:53.182328Z","shell.execute_reply":"2022-08-04T06:30:53.195685Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"train_dataset = BreastCancerDataset(train_df, transform=my_transform(key=\"train\"))\ndev_dataset = BreastCancerDataset(dev_df, transform=my_transform(key=\"val\"))\ntest_dataset = BreastCancerDataset(test_df, transform=my_transform(key=\"val\"))","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:53.202853Z","iopub.execute_input":"2022-08-04T06:30:53.203200Z","iopub.status.idle":"2022-08-04T06:30:53.209316Z","shell.execute_reply.started":"2022-08-04T06:30:53.203133Z","shell.execute_reply":"2022-08-04T06:30:53.208367Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"image_datasets = {\"train\": train_dataset, \"dev\": dev_dataset, \"test\": test_dataset}\ndataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"dev\", \"test\"]}","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:53.210989Z","iopub.execute_input":"2022-08-04T06:30:53.211280Z","iopub.status.idle":"2022-08-04T06:30:53.221831Z","shell.execute_reply.started":"2022-08-04T06:30:53.211226Z","shell.execute_reply":"2022-08-04T06:30:53.220572Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the augmentations:","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3,6,figsize=(20,11))\n\ntrain_transform = my_transform(key=\"train\", plot=True)\nval_transform = my_transform(key=\"val\", plot=True)\n\nfor m in range(6):\n    filepath = train_df.path.values[m]\n    image = Image.open(filepath)\n    ax[0,m].imshow(image)\n    transformed_img = train_transform(image)\n    ax[1,m].imshow(transformed_img)\n    ax[2,m].imshow(val_transform(image))\n    ax[0,m].grid(False)\n    ax[1,m].grid(False)\n    ax[2,m].grid(False)\n    ax[0,m].set_title(train_df.patient_id.values[m] + \"\\n target: \" + train_df.target.values[m])\n    ax[1,m].set_title(\"Preprocessing for train\")\n    ax[2,m].set_title(\"Preprocessing for val\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:53.223430Z","iopub.execute_input":"2022-08-04T06:30:53.223734Z","iopub.status.idle":"2022-08-04T06:30:55.421548Z","shell.execute_reply.started":"2022-08-04T06:30:53.223677Z","shell.execute_reply":"2022-08-04T06:30:55.420718Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"For validation we have only used the image resizing.","metadata":{}},{"cell_type":"markdown","source":"## Creating pytorch dataloaders <a class=\"anchor\" id=\"dataloaders\"></a>\n\nAs the gradients for each learning step are computed over batches we benefit from shuffling the training data after each epoch. This way each batch is composed differently and we don't start to learn for specific sequences of images. For validation and training we drop the last batch that often consists less images than the batch size.","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\ndev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:55.423328Z","iopub.execute_input":"2022-08-04T06:30:55.423674Z","iopub.status.idle":"2022-08-04T06:30:55.429239Z","shell.execute_reply.started":"2022-08-04T06:30:55.423614Z","shell.execute_reply":"2022-08-04T06:30:55.428425Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"dataloaders = {\"train\": train_dataloader, \"dev\": dev_dataloader, \"test\": test_dataloader}","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:55.430746Z","iopub.execute_input":"2022-08-04T06:30:55.431038Z","iopub.status.idle":"2022-08-04T06:30:55.443044Z","shell.execute_reply.started":"2022-08-04T06:30:55.430985Z","shell.execute_reply":"2022-08-04T06:30:55.441996Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"print(len(dataloaders[\"train\"]), len(dataloaders[\"dev\"]), len(dataloaders[\"test\"]))","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:55.445356Z","iopub.execute_input":"2022-08-04T06:30:55.445701Z","iopub.status.idle":"2022-08-04T06:30:55.457075Z","shell.execute_reply.started":"2022-08-04T06:30:55.445637Z","shell.execute_reply":"2022-08-04T06:30:55.455975Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Defining the model structure <a class=\"anchor\" id=\"model_structure\"></a>","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:55.458637Z","iopub.execute_input":"2022-08-04T06:30:55.459031Z","iopub.status.idle":"2022-08-04T06:30:55.472381Z","shell.execute_reply.started":"2022-08-04T06:30:55.458960Z","shell.execute_reply":"2022-08-04T06:30:55.471519Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Our dataset is very small and we should be afraid of overfitting to our patients. For this reason it can be a good idea to use transfer learning with a pretrained CNN. This way we benefit from the fact that the first layers of a pretrained CNN extract basic features like edges for example and only last layers contain very problem specific features. Again our policy is to start simple by using a small network like resnet18:","metadata":{}},{"cell_type":"code","source":"model = torchvision.models.resnet18(pretrained=False)\nif run_training:\n    model.load_state_dict(torch.load(\"../input/pretrained-pytorch-models/resnet18-5c106cde.pth\"))\nnum_features = model.fc.in_features\nprint(num_features)\n\nmodel.fc = nn.Sequential(\n    nn.Linear(num_features, 512),\n    nn.ReLU(),\n    nn.BatchNorm1d(512),\n    nn.Dropout(0.5),\n    \n    nn.Linear(512, 256),\n    nn.ReLU(),\n    nn.BatchNorm1d(256),\n    nn.Dropout(0.5),\n    \n    nn.Linear(256, NUM_CLASSES))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n\nmodel.apply(init_weights)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:55.473705Z","iopub.execute_input":"2022-08-04T06:30:55.474022Z","iopub.status.idle":"2022-08-04T06:30:55.753984Z","shell.execute_reply.started":"2022-08-04T06:30:55.473971Z","shell.execute_reply":"2022-08-04T06:30:55.753080Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Setting up the loss function <a class=\"anchor\" id=\"loss\"></a>\n\nThe task to predict the presence of inasive ductal carcinoma given a tissue patch is a binary classification. A common loss for this problem is the binary cross entropy function. Using it we could only use one single output neuron to make predictions. As I like to compute the f1-score during training and prediction and I need to compute false and true positives/negatives in a simple manner I will use the cross entropy loss with K=2 output neurons or 2 classes that can be hot (1) or cold (0) with $\\sum_{k}y_{nk}=1$:\n\n$$ L = -\\sum_{n=1}^{N} \\sum_{k=1}^{K=2} w_{k} \\cdot t_{n,k} \\cdot \\ln (y_{nk})$$ ","metadata":{}},{"cell_type":"markdown","source":"The $w_{k}$ are used to weight the positive and negative classes such that we are able to deal with the class imbalance. In our case we have much more negative (healthy) patches than those with cancer. To deal with this imbalance we like to increase the impact of the gradients of positive cases during training and we can do so with a higher weight than for the negative cases.","metadata":{}},{"cell_type":"code","source":"weights = compute_class_weight(y=train_df.target.values, class_weight=\"balanced\", classes=train_df.target.unique())    \nclass_weights = torch.FloatTensor(weights)\nif device.type==\"cuda\":\n    class_weights = class_weights.cuda()\nprint(class_weights)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:55.755199Z","iopub.execute_input":"2022-08-04T06:30:55.755475Z","iopub.status.idle":"2022-08-04T06:30:55.864769Z","shell.execute_reply.started":"2022-08-04T06:30:55.755419Z","shell.execute_reply":"2022-08-04T06:30:55.863884Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train_df.target.unique()","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:55.866518Z","iopub.execute_input":"2022-08-04T06:30:55.866829Z","iopub.status.idle":"2022-08-04T06:30:55.881217Z","shell.execute_reply.started":"2022-08-04T06:30:55.866772Z","shell.execute_reply":"2022-08-04T06:30:55.880273Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"You can see that class 1 (positive cancer) has a higher weight.","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(weight=class_weights)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:55.882942Z","iopub.execute_input":"2022-08-04T06:30:55.883690Z","iopub.status.idle":"2022-08-04T06:30:55.892431Z","shell.execute_reply.started":"2022-08-04T06:30:55.883626Z","shell.execute_reply":"2022-08-04T06:30:55.891335Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Selecting an evaluation metric <a class=\"anchor\" id=\"e_metric\"></a>\n\nThe percentage of correct predictions (accuracy score) is not a good evaluation metric for imbalanced target distributions. Imagine our model would predict \"no cancer\" for all patches. Even then we still obtain a accuracy score higher than 0.5 and we have no further insights. \n\n\n### The f1-score\n\nA better choice is the f1-score that is defined as the harmonic mean of precision and recall:\n\n$$ f_{1} = \\frac{2}{\\frac{1}{recall} + \\frac{1}{precision}} $$\n\nIn contrast to the arithmetic mean it is shifted towards small outiers but not influenced strongly by large ones. Hence if either the recall or the precision is small, the f1-score will then tend towards the smaller value. Let's pick an example with $recall=0.1$ and $precision=0.6$:\n\n$$ f_{1} = \\frac{2}{\\frac{1}{0.1} + \\frac{1}{0.6}} = \\frac{2}{\\frac{10}{1} + \\frac{10}{6}} = \\frac{2 \\cdot 6}{70} = \\frac{12}{70} = 0.1714$$\n\nYou can see that the $f1-score=0.1714$ is then shifted towards the smaller value of the recall and not close to the arithmetic mean of $\\mu=0.35$.\n\n### Precision and recall\n\nTo fully understand the f1-score we first need to grasp precision and recall. :-) To be honest... I always have to look it up:\n\n$$ recall = \\frac{TruePositives}{TruePositives + FalsePositives} $$\n\nImagine our model would predict \"cancer\" all the time. Then we would have the maximum number of true positives but also a very high number of false positives that decreases the recall to some baseline value. In contrast if our model would always predict \"no cancer\" we would have no true positives and our recall equals zero. For this reason one can say that the recall answers: **\"How many selected cases are positive?\"**\n\n$$ precision = \\frac{TruePositives}{TruePositives + FalseNegatives} $$\n\n**How many positive cases are selected?**","metadata":{}},{"cell_type":"code","source":"def f1_score(preds, targets):\n    \n    tp = (preds*targets).sum().to(torch.float32)\n    fp = ((1-targets)*preds).sum().to(torch.float32)\n    fn = (targets*(1-preds)).sum().to(torch.float32)\n    \n    epsilon = 1e-7\n    precision = tp / (tp + fp + epsilon)\n    recall = tp / (tp + fn + epsilon)\n    \n    f1_score = 2 * precision * recall/(precision + recall + epsilon)\n    return f1_score","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-08-04T06:30:55.894560Z","iopub.execute_input":"2022-08-04T06:30:55.895065Z","iopub.status.idle":"2022-08-04T06:30:55.904765Z","shell.execute_reply.started":"2022-08-04T06:30:55.895002Z","shell.execute_reply":"2022-08-04T06:30:55.903845Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"## Building the training loop <a class=\"anchor\" id=\"train_loop\"></a>","metadata":{}},{"cell_type":"code","source":"def train_loop(model, criterion, optimizer, lr_find=False, scheduler=None, num_epochs = 3, lam=0.0):\n    since = time.time()\n    if lr_find:\n        phases = [\"train\"]\n    else:\n        phases = [\"train\", \"dev\", \"test\"]\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    loss_dict = {\"train\": [], \"dev\": [], \"test\": []}\n    lam_tensor = torch.tensor(lam, device=device)\n    \n    running_loss_dict = {\"train\": [], \"dev\": [], \"test\": []}\n    \n    lr_find_loss = []\n    lr_find_lr = []\n    smoothing = 0.2\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        \n        for phase in phases:\n            if phase == \"train\":\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n            \n            tk0 = tqdm(dataloaders[phase], total=int(len(dataloaders[phase])))\n\n            counter = 0\n            for bi, d in enumerate(tk0):\n                inputs = d[\"image\"]\n                labels = d[\"label\"]\n                inputs = inputs.to(device, dtype=torch.float)\n                labels = labels.to(device, dtype=torch.long)\n                \n                # zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # forward\n                # track history if only in train\n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                \n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        \n                        #l2_reg = torch.tensor(0., device=device)\n                        #for param in model.parameters():\n                            #l2_reg = lam_tensor * torch.norm(param)\n                        \n                        #loss += l2_reg\n            \n                        optimizer.step()\n                        # cyclical lr schedule is invoked after each batch\n                        if scheduler is not None:\n                            scheduler.step() \n                            if lr_find:\n                                lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n                                lr_find_lr.append(lr_step)\n                                if counter==0:\n                                    lr_find_loss.append(loss.item())\n                                else:\n                                    smoothed_loss = smoothing  * loss.item() + (1 - smoothing) * lr_find_loss[-1]\n                                    lr_find_loss.append(smoothed_loss)\n                            \n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)                      \n     \n                counter += 1\n                \n                \n                tk0.set_postfix({'loss': running_loss / (counter * dataloaders[phase].batch_size),\n                                 'accuracy': running_corrects.double() / (counter * dataloaders[phase].batch_size)})\n                running_loss_dict[phase].append(running_loss / (counter * dataloaders[phase].batch_size))\n                \n            epoch_loss = running_loss / dataset_sizes[phase]\n            loss_dict[phase].append(epoch_loss)\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n            \n            # deep copy the model\n            if phase == 'dev' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n        print()\n        \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))              \n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    results = {\"model\": model,\n               \"loss_dict\": loss_dict,\n               \"running_loss_dict\": running_loss_dict,\n               \"lr_find\": {\"lr\": lr_find_lr, \"loss\": lr_find_loss}}\n    return results","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:55.906187Z","iopub.execute_input":"2022-08-04T06:30:55.906735Z","iopub.status.idle":"2022-08-04T06:30:55.931991Z","shell.execute_reply.started":"2022-08-04T06:30:55.906685Z","shell.execute_reply":"2022-08-04T06:30:55.931044Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Searching for an optimal cyclical learning rate <a class=\"anchor\" id=\"lr_cycle_optima\"></a>\n\nThe learning rate is one of the most important hyperparameters for tuning neural networks. A rate that is too high will lead to jumps to higher values in the training loss during optimization. If it's too small the learning process is too slow and will probably stop too early in the case we have defined a minimum required loss change. Take a look at the paper [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186).","metadata":{}},{"cell_type":"code","source":"start_lr = 1e-6\nend_lr = 0.1","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:55.933479Z","iopub.execute_input":"2022-08-04T06:30:55.933981Z","iopub.status.idle":"2022-08-04T06:30:55.946020Z","shell.execute_reply.started":"2022-08-04T06:30:55.933933Z","shell.execute_reply":"2022-08-04T06:30:55.945246Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def get_lr_search_scheduler(optimizer, min_lr, max_lr, max_iterations):\n    # max_iterations should be the number of steps within num_epochs_*epoch_iterations\n    # this way the learning rate increases linearily within the period num_epochs*epoch_iterations \n    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n                                               base_lr=min_lr,\n                                               max_lr=max_lr,\n                                               step_size_up=max_iterations,\n                                               step_size_down=max_iterations,\n                                               mode=\"triangular\")\n    \n    return scheduler\n\ndef get_scheduler(optimiser, min_lr, max_lr, stepsize):\n    # suggested_stepsize = 2*num_iterations_within_epoch\n    stepsize_up = np.int(stepsize/2)\n    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimiser,\n                                               base_lr=min_lr,\n                                               max_lr=max_lr,\n                                               step_size_up=stepsize_up,\n                                               step_size_down=stepsize_up,\n                                               mode=\"triangular\")\n    return scheduler","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:55.947319Z","iopub.execute_input":"2022-08-04T06:30:55.947851Z","iopub.status.idle":"2022-08-04T06:30:55.959046Z","shell.execute_reply.started":"2022-08-04T06:30:55.947805Z","shell.execute_reply":"2022-08-04T06:30:55.958151Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"import math\n\nif find_learning_rate:\n    lr_find_epochs=1\n    optimizer = optim.SGD(model.fc.parameters(), start_lr)\n    scheduler = get_lr_search_scheduler(optimizer, start_lr, end_lr, lr_find_epochs*len(train_dataloader))\n    results = train_loop(model, criterion, optimizer, lr_find=True, scheduler=scheduler, num_epochs=lr_find_epochs)\n    lr_find_lr, lr_find_loss = results[\"lr_find\"][\"lr\"], results[\"lr_find\"][\"loss\"]\n    \n    find_lr_df = pd.DataFrame(lr_find_loss, columns=[\"smoothed loss\"])\n    find_lr_df.loc[:, \"lr\"] = lr_find_lr\n    find_lr_df.to_csv(\"learning_rate_search.csv\", index=False)\nelse:\n    find_lr_df = pd.read_csv(MODEL_PATH + \"learning_rate_search.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:55.960562Z","iopub.execute_input":"2022-08-04T06:30:55.961134Z","iopub.status.idle":"2022-08-04T06:30:55.999082Z","shell.execute_reply.started":"2022-08-04T06:30:55.961076Z","shell.execute_reply":"2022-08-04T06:30:55.998161Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nax[0].plot(find_lr_df.lr.values)\nax[1].plot(find_lr_df[\"smoothed loss\"].values)\nax[0].set_xlabel(\"Steps\")\nax[0].set_ylabel(\"Learning rate\")\nax[1].set_xlabel(\"Steps\")\nax[1].set_ylabel(\"Loss\");\nax[0].set_title(\"How the learning rate increases during search\")\nax[1].set_title(\"How the training loss evolves during search\")\n\nplt.figure(figsize=(20,5))\nplt.plot(find_lr_df.lr.values, find_lr_df[\"smoothed loss\"].values, '-', color=\"tomato\");\nplt.xlabel(\"Learning rate\")\nplt.xscale(\"log\")\nplt.ylabel(\"Smoothed Loss\")\nplt.title(\"Searching for the optimal learning rate\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:56.001171Z","iopub.execute_input":"2022-08-04T06:30:56.001594Z","iopub.status.idle":"2022-08-04T06:30:57.110467Z","shell.execute_reply.started":"2022-08-04T06:30:56.001518Z","shell.execute_reply":"2022-08-04T06:30:57.109450Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"### Insights\n\n* The loss drops right from the start but increases after  values higher than 0.005. \n* Consequently we can set the minimum learning rate to 1e-6 and the maximum to 0.005.","metadata":{}},{"cell_type":"code","source":"start_lr = 1e-6\nend_lr = 0.006","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:30:57.112069Z","iopub.execute_input":"2022-08-04T06:30:57.112376Z","iopub.status.idle":"2022-08-04T06:30:57.116617Z","shell.execute_reply.started":"2022-08-04T06:30:57.112319Z","shell.execute_reply":"2022-08-04T06:30:57.115816Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"## Performing the training or loading results <a class=\"anchor\" id=\"run\"></a>","metadata":{}},{"cell_type":"code","source":"if run_training:\n    NUM_EPOCHS = 30\n    optimizer = optim.SGD(model.fc.parameters(), lr=0.01)\n    scheduler = get_scheduler(optimizer, start_lr, end_lr, 2*NUM_EPOCHS)\n    results = train_loop(model, criterion, optimizer, scheduler=scheduler, num_epochs = NUM_EPOCHS)\n    model, loss_dict, running_loss_dict = results[\"model\"], results[\"loss_dict\"], results[\"running_loss_dict\"]\n    \n    if device == \"cpu\":\n        OUTPUT_PATH += \".pth\"\n    else:\n        OUTPUT_PATH += \"_cuda.pth\"\n        \n    torch.save(model.state_dict(), OUTPUT_PATH)\n    \n    losses_df = pd.DataFrame(loss_dict[\"train\"],columns=[\"train\"])\n    losses_df.loc[:, \"dev\"] = loss_dict[\"dev\"]\n    losses_df.loc[:, \"test\"] = loss_dict[\"test\"]\n    losses_df.to_csv(\"losses_breastcancer.csv\", index=False)\n    \n    running_losses_df = pd.DataFrame(running_loss_dict[\"train\"], columns=[\"train\"])\n    running_losses_df.loc[0:len(running_loss_dict[\"dev\"])-1, \"dev\"] = running_loss_dict[\"dev\"]\n    running_losses_df.loc[0:len(running_loss_dict[\"test\"])-1, \"test\"] = running_loss_dict[\"test\"]\n    running_losses_df.to_csv(\"running_losses_breastcancer.csv\", index=False)\nelse:\n    if device == \"cpu\":\n        load_path = MODEL_PATH + \".pth\"\n    else:\n        load_path = MODEL_PATH + \"_cuda.pth\"\n    model.load_state_dict(torch.load(load_path, map_location='cpu'))\n    model.eval()\n    \n    losses_df = pd.read_csv(LOSSES_PATH + \"losses_breastcancer.csv\")\n    running_losses_df = pd.read_csv(LOSSES_PATH + \"running_losses_breastcancer.csv\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:57.118175Z","iopub.execute_input":"2022-08-04T06:30:57.118538Z","iopub.status.idle":"2022-08-04T06:30:58.492145Z","shell.execute_reply.started":"2022-08-04T06:30:57.118470Z","shell.execute_reply":"2022-08-04T06:30:58.491246Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# Exploring results and errors <a class=\"anchor\" id=\"error_analysis\"></a>","metadata":{}},{"cell_type":"markdown","source":"## Loss convergence <a class=\"anchor\" id=\"losses\"></a>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\n\nplt.plot(losses_df[\"train\"], '-o', label=\"train\")\nplt.plot(losses_df[\"dev\"], '-o', label=\"dev\")\nplt.plot(losses_df[\"test\"], '-o', label=\"dev\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Weighted x-entropy\")\nplt.title(\"Loss change over epoch\")\nplt.legend();","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:58.493571Z","iopub.execute_input":"2022-08-04T06:30:58.493884Z","iopub.status.idle":"2022-08-04T06:30:58.780496Z","shell.execute_reply.started":"2022-08-04T06:30:58.493828Z","shell.execute_reply":"2022-08-04T06:30:58.779681Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(3,1,figsize=(20,15))\n\nax[0].plot(running_losses_df[\"train\"], '-o', label=\"train\")\nax[0].set_xlabel(\"Step\")\nax[0].set_ylabel(\"Weighted x-entropy\")\nax[0].set_title(\"Loss change over steps\")\nax[0].legend();\n\nax[1].plot(running_losses_df[\"dev\"], '-o', label=\"dev\", color=\"orange\")\nax[1].set_xlabel(\"Step\")\nax[1].set_ylabel(\"Weighted x-entropy\")\nax[1].set_title(\"Loss change over steps\")\nax[1].legend();\n\nax[2].plot(running_losses_df[\"test\"], '-o', label=\"test\", color=\"mediumseagreen\")\nax[2].set_xlabel(\"Step\")\nax[2].set_ylabel(\"Weighted x-entropy\")\nax[2].set_title(\"Loss change over steps\")\nax[2].legend();","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:30:58.781726Z","iopub.execute_input":"2022-08-04T06:30:58.782182Z","iopub.status.idle":"2022-08-04T06:31:01.361608Z","shell.execute_reply.started":"2022-08-04T06:30:58.782120Z","shell.execute_reply":"2022-08-04T06:31:01.360826Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"## The probability landscape of invasive ductal carcinoma <a class=\"anchor\" id=\"landscape\"></a>","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    return 1./(1+np.exp(-x))\n\ndef evaluate_model(model, predictions_df, key):\n    was_training = model.training\n    model.eval()\n\n    with torch.no_grad():\n        for i, data in enumerate(dataloaders[key]):\n            inputs = data[\"image\"].to(device)\n            labels = data[\"label\"].to(device)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            proba = outputs.cpu().numpy().astype(np.float)\n            predictions_df.loc[i*BATCH_SIZE:(i+1)*BATCH_SIZE-1, \"proba\"] = sigmoid(proba[:, 1])\n            predictions_df.loc[i*BATCH_SIZE:(i+1)*BATCH_SIZE-1, \"true\"] = data[\"label\"].numpy().astype(np.int)\n            predictions_df.loc[i*BATCH_SIZE:(i+1)*BATCH_SIZE-1, \"predicted\"] = preds.cpu().numpy().astype(np.int)\n            predictions_df.loc[i*BATCH_SIZE:(i+1)*BATCH_SIZE-1, \"x\"] = data[\"x\"].numpy()\n            predictions_df.loc[i*BATCH_SIZE:(i+1)*BATCH_SIZE-1, \"y\"] = data[\"y\"].numpy()\n            predictions_df.loc[i*BATCH_SIZE:(i+1)*BATCH_SIZE-1, \"patient_id\"] = data[\"patient_id\"]\n            \n    predictions_df = predictions_df.dropna()\n    return predictions_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:31:01.362864Z","iopub.execute_input":"2022-08-04T06:31:01.363302Z","iopub.status.idle":"2022-08-04T06:31:01.376083Z","shell.execute_reply.started":"2022-08-04T06:31:01.363241Z","shell.execute_reply":"2022-08-04T06:31:01.374921Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"if run_training:\n    dev_predictions = pd.DataFrame(index = np.arange(0, dataset_sizes[\"dev\"]), columns = [\"true\", \"predicted\", \"proba\"])\n    test_predictions = pd.DataFrame(index = np.arange(0, dataset_sizes[\"test\"]), columns = [\"true\", \"predicted\", \"proba\"])\n\n    dev_predictions = evaluate_model(model, dev_predictions, \"dev\")\n    test_predictions = evaluate_model(model, test_predictions, \"test\")\n    \n    dev_predictions.to_csv(\"dev_predictions.csv\", index=False)\n    test_predictions.to_csv(\"test_predictions.csv\", index=False)\n    \nelse:\n    \n    dev_predictions = pd.read_csv(LOSSES_PATH + \"dev_predictions.csv\")\n    test_predictions = pd.read_csv(LOSSES_PATH + \"test_predictions.csv\")\n    \n    dev_predictions.patient_id = dev_predictions.patient_id.astype(np.str)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:31:01.377603Z","iopub.execute_input":"2022-08-04T06:31:01.377904Z","iopub.status.idle":"2022-08-04T06:31:01.537197Z","shell.execute_reply.started":"2022-08-04T06:31:01.377851Z","shell.execute_reply":"2022-08-04T06:31:01.536252Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(3,3,figsize=(20,20))\n\nfor n in range(3):\n\n    idx = dev_predictions.patient_id.unique()[n]\n    grid, mask, broken_patches, mask_proba = visualise_breast_tissue(idx, pred_df=dev_predictions)\n\n\n    ax[n, 0].imshow(grid, alpha=0.9)\n    ax[n, 1].imshow(mask, alpha=0.8)\n    ax[n, 1].imshow(grid, alpha=0.7)\n    ax[n, 2].imshow(mask_proba[:,:,0], cmap=\"YlOrRd\")\n\n    for m in range(3):\n        ax[n, m].set_xlabel(\"y-coord\")\n        ax[n, m].set_ylabel(\"x-coord\")\n        ax[n, m].grid(False)\n        \n    ax[n, 0].set_title(\"Breast tissue slice of patient: \" + patient_id)\n    ax[n, 1].set_title(\"Cancer tissue colored red \\n of patient: \" + patient_id);\n    ax[n, 2].set_title(\"Cancer probability\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:31:01.538600Z","iopub.execute_input":"2022-08-04T06:31:01.539018Z","iopub.status.idle":"2022-08-04T06:31:43.232979Z","shell.execute_reply.started":"2022-08-04T06:31:01.538834Z","shell.execute_reply":"2022-08-04T06:31:43.231853Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"## Going into details\n\n### TODO\n\n* Number of correct classifications per patient compared to cancer/tissue size.\n* Confusion matrix\n* Exploring very bad results. ","metadata":{}},{"cell_type":"code","source":"dev_predictions.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:31:43.234699Z","iopub.execute_input":"2022-08-04T06:31:43.235062Z","iopub.status.idle":"2022-08-04T06:31:43.252208Z","shell.execute_reply.started":"2022-08-04T06:31:43.235000Z","shell.execute_reply":"2022-08-04T06:31:43.251096Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,3,figsize=(20,5))\nsns.countplot(dev_predictions.true.astype(np.float), ax=ax[0], palette=\"Reds_r\")\nax[0].set_title(\"Target counts of dev data\");\nsns.distplot(dev_predictions.proba.astype(np.float), ax=ax[1], kde=False, color=\"tomato\")\nax[0].set_title(\"Predicted probability of cancer in dev\");\nsns.distplot(test_predictions.proba.astype(np.float), ax=ax[2], kde=False, color=\"mediumseagreen\");\nax[2].set_title(\"Predicted probability of cancer in test\");","metadata":{"execution":{"iopub.status.busy":"2022-08-04T06:31:43.253504Z","iopub.execute_input":"2022-08-04T06:31:43.253831Z","iopub.status.idle":"2022-08-04T06:31:43.939731Z","shell.execute_reply.started":"2022-08-04T06:31:43.253779Z","shell.execute_reply":"2022-08-04T06:31:43.938755Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ndef get_confusion_matrix(y_true, y_pred):\n    transdict = {1: \"cancer\", 0: \"no cancer\"}\n    y_t = np.array([transdict[x] for x in y_true])\n    y_p = np.array([transdict[x] for x in y_pred])\n    \n    labels = [\"no cancer\", \"cancer\"]\n    index_labels = [\"actual no cancer\", \"actual cancer\"]\n    col_labels = [\"predicted no cancer\", \"predicted cancer\"]\n    confusion = confusion_matrix(y_t, y_p, labels=labels)\n    confusion_df = pd.DataFrame(confusion, index=index_labels, columns=col_labels)\n    for n in range(2):\n        confusion_df.iloc[n] = confusion_df.iloc[n] / confusion_df.sum(axis=1).iloc[n]\n    return confusion_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-04T06:34:00.827849Z","iopub.execute_input":"2022-08-04T06:34:00.828235Z","iopub.status.idle":"2022-08-04T06:34:00.836937Z","shell.execute_reply.started":"2022-08-04T06:34:00.828186Z","shell.execute_reply":"2022-08-04T06:34:00.835896Z"},"trusted":true},"execution_count":59,"outputs":[]}]}